{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üè• NurseSim-Triage: LLM-as-Judge Evaluation\n",
                "\n",
                "**Using GPT-5, Gemini 3, and Gemini 3 Pro**\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q openai google-generativeai pandas numpy scikit-learn matplotlib tqdm"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os, json, time, re\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "from typing import Dict, List, Any\n",
                "from dataclasses import dataclass\n",
                "from enum import Enum\n",
                "import matplotlib.pyplot as plt\n",
                "from tqdm.auto import tqdm\n",
                "from sklearn.metrics import cohen_kappa_score\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "import openai\n",
                "import google.generativeai as genai\n",
                "print(\"‚úÖ Imports done\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import userdata\n",
                "openai.api_key = userdata.get('OPENAI_API_KEY')\n",
                "genai.configure(api_key=userdata.get('GOOGLE_API_KEY'))\n",
                "print(\"‚úÖ API keys set\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class TriageCategory(Enum):\n",
                "    IMMEDIATE = 1\n",
                "    VERY_URGENT = 2\n",
                "    URGENT = 3\n",
                "    STANDARD = 4\n",
                "    NON_URGENT = 5\n",
                "\n",
                "@dataclass\n",
                "class EvaluationCriteria:\n",
                "    triage_accuracy: int\n",
                "    clinical_reasoning: int\n",
                "    safety_assessment: str\n",
                "    communication_quality: int\n",
                "    efficiency: int\n",
                "    confidence: float\n",
                "    justification: str\n",
                "\n",
                "@dataclass\n",
                "class PatientScenario:\n",
                "    scenario_id: str\n",
                "    description: str\n",
                "    vital_signs: Dict[str, Any]\n",
                "    presenting_complaint: str\n",
                "    medical_history: str\n",
                "    expected_triage: TriageCategory\n",
                "    expected_reasoning: str\n",
                "    difficulty: str\n",
                "\n",
                "@dataclass\n",
                "class ModelResponse:\n",
                "    scenario_id: str\n",
                "    triage_decision: str\n",
                "    response_time_ms: int\n",
                "\n",
                "print(\"‚úÖ Data structures\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "TEST_SCENARIOS = [\n",
                "    PatientScenario(\"EASY_001\", \"72M chest pain to L arm\", {\"HR\": 110, \"BP\": \"160/95\", \"RR\": 24, \"SpO2\": 94, \"Temp\": 37.2, \"AVPU\": \"A\"}, \"Crushing chest pain 30min, sweating, nausea\", \"HTN, T2DM, MI 2019\", TriageCategory.IMMEDIATE, \"Classic ACS\", \"EASY\"),\n",
                "    PatientScenario(\"EASY_002\", \"25F sore throat 3d\", {\"HR\": 78, \"BP\": \"118/72\", \"RR\": 14, \"SpO2\": 99, \"Temp\": 37.8, \"AVPU\": \"A\"}, \"Sore throat, mild dysphagia\", \"No PMH\", TriageCategory.NON_URGENT, \"Viral pharyngitis\", \"EASY\"),\n",
                "    PatientScenario(\"MED_001\", \"45F abdo pain + fever\", {\"HR\": 98, \"BP\": \"128/82\", \"RR\": 18, \"SpO2\": 98, \"Temp\": 38.6, \"AVPU\": \"A\"}, \"RLQ pain 12h, worsening\", \"Appendectomy 22yo\", TriageCategory.URGENT, \"Possible surgical abdomen\", \"MEDIUM\"),\n",
                "    PatientScenario(\"MED_002\", \"68M confusion + cough\", {\"HR\": 102, \"BP\": \"105/65\", \"RR\": 22, \"SpO2\": 92, \"Temp\": 38.9, \"AVPU\": \"V\"}, \"Confusion 6h, green sputum\", \"COPD\", TriageCategory.VERY_URGENT, \"Sepsis/CAP\", \"MEDIUM\"),\n",
                "    PatientScenario(\"HARD_001\", \"32F headache + neck stiff\", {\"HR\": 88, \"BP\": \"122/78\", \"RR\": 16, \"SpO2\": 99, \"Temp\": 38.2, \"AVPU\": \"A\"}, \"Worst headache ever, photophobia\", \"Migraine hx\", TriageCategory.IMMEDIATE, \"Meningitis/SAH\", \"HARD\"),\n",
                "    PatientScenario(\"HARD_002\", \"55M diabetic foot\", {\"HR\": 92, \"BP\": \"145/88\", \"RR\": 16, \"SpO2\": 97, \"Temp\": 37.4, \"AVPU\": \"A\"}, \"Non-healing ulcer 2wk, redness\", \"T2DM 15y\", TriageCategory.URGENT, \"Diabetic foot infection\", \"HARD\"),\n",
                "    PatientScenario(\"EXP_001\", \"78F vague malaise\", {\"HR\": 72, \"BP\": \"138/84\", \"RR\": 18, \"SpO2\": 96, \"Temp\": 36.8, \"AVPU\": \"A\"}, \"Not feeling right 2d\", \"HTN\", TriageCategory.VERY_URGENT, \"Atypical MI elderly female\", \"EXPERT\"),\n",
                "    PatientScenario(\"EXP_002\", \"19M agitated nightclub\", {\"HR\": 125, \"BP\": \"155/105\", \"RR\": 22, \"SpO2\": 98, \"Temp\": 38.8, \"AVPU\": \"A\"}, \"Agitation, sweating, dilated pupils\", \"Unknown\", TriageCategory.IMMEDIATE, \"Toxidrome\", \"EXPERT\"),\n",
                "]\n",
                "print(f\"‚úÖ {len(TEST_SCENARIOS)} scenarios\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Triage model using Gemini 3 Pro\n",
                "TRIAGE_PROMPT = \"\"\"You are NurseSim-Triage. Provide: 1) TRIAGE CATEGORY (1-5), 2) REASONING, 3) ACTIONS\"\"\"\n",
                "\n",
                "GEMINI_MODELS = ['gemini-3-pro', 'gemini-3', 'gemini-2.5-pro', 'gemini-2.0-flash-exp', 'gemini-1.5-pro']\n",
                "triage_model = None\n",
                "ACTIVE_MODEL = None\n",
                "\n",
                "for m in GEMINI_MODELS:\n",
                "    try:\n",
                "        triage_model = genai.GenerativeModel(m, system_instruction=TRIAGE_PROMPT)\n",
                "        triage_model.generate_content(\"test\")\n",
                "        ACTIVE_MODEL = m\n",
                "        print(f\"‚úÖ Triage: {m}\")\n",
                "        break\n",
                "    except Exception as e:\n",
                "        print(f\"   {m}: {str(e)[:40]}\")\n",
                "\n",
                "def generate_triage(s):\n",
                "    prompt = f\"Patient: {s.description}\\nComplaint: {s.presenting_complaint}\\nVitals: {s.vital_signs}\\nHistory: {s.medical_history}\"\n",
                "    start = time.time()\n",
                "    try:\n",
                "        r = triage_model.generate_content(prompt)\n",
                "        return ModelResponse(s.scenario_id, r.text, int((time.time()-start)*1000))\n",
                "    except Exception as e:\n",
                "        return ModelResponse(s.scenario_id, f\"[Error: {e}]\", 0)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test triage\n",
                "test = generate_triage(TEST_SCENARIOS[0])\n",
                "print(f\"‚úÖ Test: {test.response_time_ms}ms\\n{test.triage_decision[:300]}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Judges\n",
                "JUDGE_PROMPT = \"Expert Triage Nurse. Rate 1-5: triage_accuracy, clinical_reasoning, communication_quality, efficiency. safety_assessment: PASS/CONCERN/FAIL. confidence: 0-1. Return JSON only.\"\n",
                "\n",
                "def parse_eval(raw):\n",
                "    try:\n",
                "        m = re.search(r'\\{[^{}]*\\}', raw, re.DOTALL)\n",
                "        if m:\n",
                "            d = json.loads(m.group())\n",
                "            return EvaluationCriteria(int(d.get('triage_accuracy',3)), int(d.get('clinical_reasoning',3)), d.get('safety_assessment','CONCERN'), int(d.get('communication_quality',3)), int(d.get('efficiency',3)), float(d.get('confidence',0.5)), d.get('justification',''))\n",
                "    except: pass\n",
                "    return EvaluationCriteria(3,3,\"CONCERN\",3,3,0.0,\"parse error\")\n",
                "\n",
                "def judge_prompt(s, r):\n",
                "    return f\"EXPECTED: {s.expected_triage.name} - {s.expected_reasoning}\\nMODEL: {r.triage_decision}\\nReturn JSON.\"\n",
                "\n",
                "class GPT5Judge:\n",
                "    def __init__(self):\n",
                "        self.client = openai.OpenAI()\n",
                "        self.models = ['gpt-5', 'gpt-4.5-turbo', 'gpt-4o', 'gpt-4-turbo']\n",
                "        self.name = \"GPT-5\"\n",
                "    def evaluate(self, s, r):\n",
                "        for m in self.models:\n",
                "            try:\n",
                "                resp = self.client.chat.completions.create(model=m, messages=[{\"role\":\"system\",\"content\":JUDGE_PROMPT},{\"role\":\"user\",\"content\":judge_prompt(s,r)}], temperature=0.2)\n",
                "                self.name = m.upper()\n",
                "                return parse_eval(resp.choices[0].message.content)\n",
                "            except: continue\n",
                "        return parse_eval(\"{}\")\n",
                "\n",
                "class Gemini3Judge:\n",
                "    def __init__(self):\n",
                "        self.models = ['gemini-3', 'gemini-2.5-pro', 'gemini-2.0-flash-exp', 'gemini-1.5-pro']\n",
                "        self.name = \"Gemini-3\"\n",
                "        self.model = None\n",
                "        for m in self.models:\n",
                "            try:\n",
                "                self.model = genai.GenerativeModel(m, system_instruction=JUDGE_PROMPT)\n",
                "                self.model.generate_content(\"test\")\n",
                "                self.name = m.upper()\n",
                "                break\n",
                "            except: continue\n",
                "    def evaluate(self, s, r):\n",
                "        if not self.model: return parse_eval(\"{}\")\n",
                "        try:\n",
                "            return parse_eval(self.model.generate_content(judge_prompt(s,r)).text)\n",
                "        except: return parse_eval(\"{}\")\n",
                "\n",
                "class Gemini3ProJudge:\n",
                "    def __init__(self):\n",
                "        self.models = ['gemini-3-pro', 'gemini-2.5-pro', 'gemini-2.0-flash-exp']\n",
                "        self.name = \"Gemini-3-Pro\"\n",
                "        self.model = None\n",
                "        for m in self.models:\n",
                "            try:\n",
                "                self.model = genai.GenerativeModel(m, system_instruction=JUDGE_PROMPT)\n",
                "                self.model.generate_content(\"test\")\n",
                "                self.name = m.upper()\n",
                "                break\n",
                "            except: continue\n",
                "    def evaluate(self, s, r):\n",
                "        if not self.model: return parse_eval(\"{}\")\n",
                "        try:\n",
                "            return parse_eval(self.model.generate_content(judge_prompt(s,r)).text)\n",
                "        except: return parse_eval(\"{}\")\n",
                "\n",
                "print(\"‚úÖ Judge classes\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize\n",
                "judges = [GPT5Judge(), Gemini3Judge(), Gemini3ProJudge()]\n",
                "for j in judges:\n",
                "    print(f\"  {j.name}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run evaluation\n",
                "print(\"üöÄ Evaluating...\\n\")\n",
                "results = []\n",
                "for s in tqdm(TEST_SCENARIOS):\n",
                "    r = generate_triage(s)\n",
                "    row = {'id': s.scenario_id, 'difficulty': s.difficulty, 'expected': s.expected_triage.name}\n",
                "    for j in judges:\n",
                "        ev = j.evaluate(s, r)\n",
                "        row[f'{j.name}_acc'] = ev.triage_accuracy\n",
                "        row[f'{j.name}_reason'] = ev.clinical_reasoning\n",
                "        row[f'{j.name}_safety'] = ev.safety_assessment\n",
                "        time.sleep(0.3)\n",
                "    results.append(row)\n",
                "df = pd.DataFrame(results)\n",
                "print(\"\\n‚úÖ Done!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Results\n",
                "print(\"üìä RESULTS\")\n",
                "print(\"=\"*50)\n",
                "for j in judges:\n",
                "    col = f'{j.name}_acc'\n",
                "    if col in df.columns:\n",
                "        print(f\"{j.name}: {df[col].mean():.2f}/5 | Safety PASS: {(df[f'{j.name}_safety']=='PASS').sum()}/{len(df)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Chart\n",
                "valid = [j.name for j in judges if f'{j.name}_acc' in df.columns]\n",
                "acc = [df[f'{j}_acc'].mean() for j in valid]\n",
                "plt.figure(figsize=(8,5))\n",
                "plt.bar(valid, acc, color=['#3b82f6','#10b981','#8b5cf6'])\n",
                "plt.ylabel('Mean Accuracy')\n",
                "plt.title('Triage Accuracy: GPT-5 vs Gemini 3 vs Gemini 3 Pro')\n",
                "plt.ylim(0,5)\n",
                "for i,v in enumerate(acc): plt.text(i, v+0.1, f'{v:.2f}', ha='center', fontweight='bold')\n",
                "plt.savefig('gemini3_evaluation.png', dpi=150)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df.to_csv('nursesim_gemini3_eval.csv', index=False)\n",
                "print(\"‚úÖ Saved: nursesim_gemini3_eval.csv\")\n",
                "print(f\"\\nüè• Overall: {np.mean(acc):.2f}/5\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}